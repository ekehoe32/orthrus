

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>datasci.sparse.feature_selection namespace &mdash; DataSci 1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="datasci.sparse.classifiers namespace" href="datasci.sparse.classifiers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DataSci
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">datasci</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="datasci.html">datasci namespace</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="datasci.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="datasci.core.html">datasci.core namespace</a></li>
<li class="toctree-l4"><a class="reference internal" href="datasci.preprocessing.html">datasci.preprocessing namespace</a></li>
<li class="toctree-l4"><a class="reference internal" href="datasci.decomposition.html">datasci.decomposition namespace</a></li>
<li class="toctree-l4"><a class="reference internal" href="datasci.manifold.html">datasci.manifold namespace</a></li>
<li class="toctree-l4"><a class="reference internal" href="datasci.model_selection.html">datasci.model_selection namespace</a></li>
<li class="toctree-l4"><a class="reference internal" href="datasci.solvers.html">datasci.solvers namespace</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="datasci.sparse.html">datasci.sparse namespace</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataSci</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="modules.html">datasci</a> &raquo;</li>
        
          <li><a href="datasci.html">datasci namespace</a> &raquo;</li>
        
          <li><a href="datasci.sparse.html">datasci.sparse namespace</a> &raquo;</li>
        
      <li>datasci.sparse.feature_selection namespace</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/rst/datasci.sparse.feature_selection.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="datasci-sparse-feature-selection-namespace">
<h1>datasci.sparse.feature_selection namespace<a class="headerlink" href="#datasci-sparse-feature-selection-namespace" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-datasci.sparse.feature_selection.IterativeFeatureRemoval">
<span id="datasci-sparse-feature-selection-iterativefeatureremoval-module"></span><h2>datasci.sparse.feature_selection.IterativeFeatureRemoval module<a class="headerlink" href="#module-datasci.sparse.feature_selection.IterativeFeatureRemoval" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.IterativeFeatureRemoval.</span></code><code class="sig-name descname"><span class="pre">IFR</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">classifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repetition</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partition_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'stratified_k-fold'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nfolds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.75</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jumpratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features_per_iter_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbosity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The Iterative Feature Removal algorithms extracts features for many data partitions independently, and combines
them and keep track of feature frequency, weights and iterations in which each feature was extracted. During execution,
the data is partitioned into training and validation sets ‘repetition’ number of times and then for each partition
one feature set is extracted. So, a total of repetition * num_partitions independent feature sets are extracted
and the results are merged to create the output. These independent feature set extractions are batched
and run in parallel using python ray package.</p>
<p>For each feature set, the algorithm can halt because of the following conditions:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>BSR on validation partition is below cutoff</p></li>
<li><p>Jump does not occur in the array of sorted absolute weights</p></li>
<li><p>Jump occurs but the weight at the jump is too small ( &lt; 10e-6)</p></li>
<li><p>Number of features selected for the current iteration is greater than max_features_per_iter_ratio * num_samples in training partition. This condition prevents overfitting.</p></li>
<li><p>max_iters number of iterations complete successfully</p></li>
</ol>
</div></blockquote>
<p>When one of these conditions happen, further feature extraction on the current fold is stopped.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>classifier</strong> (<em>object</em>) – Classifier to run the classification experiment with; must have the sklearn equivalent
of a <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> method.</p></li>
<li><p><strong>repetition</strong> (<em>int</em>) – Determines the number of times to partition the dataset. (default: 10)</p></li>
<li><p><strong>partition_method</strong> (<em>string</em>) – A partition method that is compatible with calcom.utils.generate_partitions (default: ‘stratified_k-fold’)</p></li>
<li><p><strong>nfolds</strong> (<em>int</em>) – The number of folds to partition data into (default: 3)</p></li>
<li><p><strong>max_iters</strong> (<em>int</em>) – Determines the maximum number of iterations of IFR on one data partition(default: 5)</p></li>
<li><p><strong>cutoff</strong> (<em>float</em>) – Threshold for the validation BSR (balanced success rate) to halt the process. (default: 0.75)</p></li>
<li><p><strong>jumpratio</strong> (<em>float</em>) – The relative drop in the magnitude of coefficients in weight vector to identify numerically zero weights (default: 100)</p></li>
<li><p><strong>max_features_per_iter_ratio</strong> (<em>float</em>) – A fraction that limits the max number of features that can be extracted per iteration. (default: 0.8)
if the number if selected features is greater than max_features_per_iter_ratio * #samples in training partition, further execution
on the current fold is stopped.</p></li>
<li><p><strong>verbosity</strong> (<em>int</em>) – Determines verbosity of print statments; 0 for no output; 2 for full output. (default: 0)</p></li>
<li><p><strong>max_processes</strong> (<em>int</em>) – Defines the maximum number of jobs to run in parallel. It is intended to be used
in resource constrained situation; lower the number of process if you are getting errors/exceptions</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.diagnostic_information_">
<code class="sig-name descname"><span class="pre">diagnostic_information_</span></code><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.diagnostic_information_" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds execution information for each interation of each partition.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dataset_dirpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">consts</span><span class="o">.</span><span class="n">DATA_SET_LOCATION</span><span class="p">,</span> <span class="s1">&#39;metabolomics&#39;</span><span class="p">,</span> <span class="s1">&#39;urine&#39;</span><span class="p">,</span> <span class="s1">&#39;adult_urine&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dirpath</span><span class="p">,</span> <span class="s1">&#39;DoD-Urine-Adult_skyline-list-for-Eric_20200605_adjusted.ds&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">calcom</span><span class="o">.</span><span class="n">classifiers</span><span class="o">.</span><span class="n">SSVMClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;method&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">calcom</span><span class="o">.</span><span class="n">solvers</span><span class="o">.</span><span class="n">LPPrimalDualPy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;use_cuda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    model,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_features_per_iter_ratio = 2,</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    )</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#see feature select method for details</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.fit" title="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">IFR.fit()</span></code></a> to understand the output of <cite>IFR</cite>.</p>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Args:
data (ndarray of shape (m, n))): array of data, with m the number of observations in R^n.
labels (ndarray of shape (m))): vector of labels for the data</p>
<p>Return:
(pandas.DataFrame) : The dataframe contains the results of IFR. It is indexed by feature_ids and each column
contains different information for each feature as described below:</p>
<blockquote>
<div><ul class="simple">
<li><p>frequency : How many times the feature is extracted</p></li>
<li><p>weights : Contains a list of weights, from the weight vectors during training on different partitions.
Each value corresponds to the weight for the feature over different extractions. The length of the weights
is equal to the frequency.</p></li>
<li><p>selection_iteration : Contains a list of indices of the iteration when the feature was extracted over
different data partitions. The length of the list is equal to the frequency.</p></li>
</ul>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.plot_basic_diagnostic_stats">
<code class="sig-name descname"><span class="pre">plot_basic_diagnostic_stats</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">validation_bsr_iteration_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_random_exp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.plot_basic_diagnostic_stats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.select_features_for_data_partition">
<code class="sig-name descname"><span class="pre">select_features_for_data_partition</span></code><em class="property"> <span class="pre">=</span> <span class="pre">&lt;ray.remote_function.RemoteFunction</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#datasci.sparse.feature_selection.IterativeFeatureRemoval.IFR.select_features_for_data_partition" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.helper">
<span id="datasci-sparse-feature-selection-helper-module"></span><h2>datasci.sparse.feature_selection.helper module<a class="headerlink" href="#module-datasci.sparse.feature_selection.helper" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.rank_features_by_attribute">
<code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.helper.</span></code><code class="sig-name descname"><span class="pre">rank_features_by_attribute</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.rank_features_by_attribute" title="Permalink to this definition">¶</a></dt>
<dd><p>This method takes a features dataframe as input and ranks the features based on a numerical column/attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features_df</strong> (<em>pandas.DataFrame</em>) – This is a features dataframe that contains result of a feature selection.
(check datasci.core.dataset.DataSet.feature_select method for details)</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>This dictionary contains variables to determine which attribute to rank feature on and the
order of ranking. Check details for various key and values below:
‘attr’ (Mandatory): Attribute/column name from features_df to rank the features on
‘order’: Whether to rank in ascending or descending order. ‘asc’ for ascending and ‘desc’ for descending.</p>
<blockquote>
<div><p>(defaul: ‘desc’)</p>
</div></blockquote>
<dl class="simple">
<dt>’feature_ids’ (list-like): List of identifiers for the features to limit the ranking within certain features only.</dt><dd><p>e.g. [True, False, True], [‘gene1’, ‘gene3’], etc…, can also be pandas series or numpy array.
Default: None, which corresponds to using all features.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>n = number of features in feature_ids or features_df (if feature_ids is None). It contains sorted feature ids (index of features_df).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n, ))</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features_df</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_results&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_subset</span> <span class="o">=</span> <span class="n">features_df</span><span class="p">[</span><span class="s1">&#39;frequency&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ranking_method_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attr&#39;</span><span class="p">:</span> <span class="s1">&#39;frequency&#39;</span><span class="p">,</span> <span class="n">feature_ids</span><span class="p">:</span> <span class="n">feature_subset</span><span class="p">}</span>
</pre></div>
</div>
<p>feature will ranked on frequency attribute in descending and will occur only within feature_subset features.
&gt;&gt;&gt; ranked_feature_ids =  rank_features_by_attribute(features_df, ranking_method_args)</p>
</dd></dl>

<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.rank_features_by_mean_attribute_value">
<code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.helper.</span></code><code class="sig-name descname"><span class="pre">rank_features_by_mean_attribute_value</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.rank_features_by_mean_attribute_value" title="Permalink to this definition">¶</a></dt>
<dd><p>This method takes a features dataframe as input and ranks the features based on the
mean/meadian value a column/attribute, which contains list of numerical data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features_df</strong> (<em>pandas.DataFrame</em>) – This is a features dataframe that contains result of a feature selection.
(check datasci.core.dataset.DataSet.feature_select method for details)</p></li>
<li><p><strong>args</strong> (<em>dict</em>) – <p>This dictionary contains variables to determine which attribute to rank feature on and the
order of ranking. Check details for various key and values below:
‘attr’ (Mandatory): Attribute/column name from features_df to rank the features on
‘order’: Whether to rank in ascending or descending order. ‘asc’ for ascending and ‘desc’ for descending.</p>
<blockquote>
<div><p>(defaul: ‘desc’)</p>
</div></blockquote>
<dl class="simple">
<dt>’feature_ids’ (list-like): List of identifiers for the features to limit the ranking within certain features only.</dt><dd><p>e.g. [True, False, True], [‘gene1’, ‘gene3’], etc…, can also be pandas series or numpy array.
Default: None, which corresponds to using all features.</p>
</dd>
</dl>
<p>’method’: which operation to perform, can be “mean” or “median”. (Default: “mean”)</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>n = number of features in feature_ids or features_df (if feature_ids is None). It contains sorted feature ids (index of features_df).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n, ))</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features_df</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_results&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ranking_method_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attr&#39;</span><span class="p">:</span> <span class="s1">&#39;selection_iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;order&#39;</span><span class="p">:</span> <span class="n">asc</span><span class="p">,</span> <span class="s1">&#39;method&#39;</span><span class="p">:</span> <span class="s1">&#39;median&#39;</span><span class="p">}</span>
<span class="go">The selection_iteration column in the features_df contains a list of integer values.</span>
<span class="go">In this example the features will be ranked by the median value of selection_iteration in ascending order.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ranked_feature_ids</span> <span class="o">=</span>  <span class="n">rank_features_by_mean_attribute_value</span><span class="p">(</span><span class="n">features_df</span><span class="p">,</span> <span class="n">ranking_method_args</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.rank_features_within_attribute_class">
<code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.helper.</span></code><code class="sig-name descname"><span class="pre">rank_features_within_attribute_class</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_class_attribute</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_feature_attribute_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partitioner</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scorer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classification_attr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classifier_factory_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_weights_handle</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.rank_features_within_attribute_class" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.reduce_feature_set_size">
<code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.helper.</span></code><code class="sig-name descname"><span class="pre">reduce_feature_set_size</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">features_dataframe</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attr</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scorer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranking_method_handle</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranking_method_args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partitioner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_sample_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jump</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose_frequency</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.reduce_feature_set_size" title="Permalink to this definition">¶</a></dt>
<dd><p>This method takes a features dataframe (output of a feature selection), ranks them by a ranking method and performs
a feature set reduction using grid search method, which is defined by start, end and jump parameters.</p>
<p>Different training and test data may be used and the results will change accordingly. The following choices are available:</p>
<p>if test_sample_ids is None and partitioner is None:</p>
<blockquote>
<div><p>Only training data is available, so the results will contain score on the Training data</p>
</div></blockquote>
<p>if test_sample_ids is not None and partitioner is None:</p>
<blockquote>
<div><p>Model is trained on all sample_ids, and then it is then evaluated on test_sample_ids. Results contain evaluation score on test_sample_ids.</p>
</div></blockquote>
<p>if test_sample_ids is None and partitioner is not None:</p>
<blockquote>
<div><p>Model is trained on partitions of sample_ids created by partitioner; results will contain the mean validation score,
obtained during validation on these different partitions.</p>
</div></blockquote>
<p>if test_sample_ids is not None and partitioner is not None:</p>
<blockquote>
<div><p>Model is trained using partitions of sample_ids defined by partitioner, and then the test_sample_ids are evaluated using all models. Results
contain mean evaluation score on test_sample_ids.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features_df</strong> (<em>pandas.DataFrame</em>) – This is a features dataframe that contains result of a feature selection.
(check datasci.core.dataset.DataSet.feature_select method for details)</p></li>
<li><p><strong>sample_ids</strong> (<em>like-like</em>) – List of indicators for the samples to use for training. e.g. [1,3], [True, False, True],
[‘human1’, ‘human3’], etc…, can also be pandas series or numpy array.</p></li>
<li><p><strong>attr</strong> (<em>string</em>) – Name of metadata attribute to classify on.</p></li>
<li><p><strong>classifier</strong> (<em>object</em>) – Classifier to run the classification experiment with; must have the sklearn equivalent
of a <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> method.</p></li>
<li><p><strong>scorer</strong> (<em>object</em>) – Function which scores the prediction labels on training and test partitions. This function
should accept two arguments: truth labels and prediction labels. This function should output a score
between 0 and 1 which can be thought of as an accuracy measure. See
sklearn.metrics.balanced_accuracy_score for an example.</p></li>
<li><p><strong>ranking_method_handle</strong> (<em>method handle</em>) – handle of the feature ranking method</p></li>
<li><p><strong>ranking_method_args</strong> (<em>dict</em>) – argument dictionary for the feature ranking method</p></li>
<li><p><strong>partitioner</strong> (<em>object</em>) – Class-instance which partitions samples in batches of training and test split. This
instance must have the sklearn equivalent of a split method. The split method returns a list of
train-test partitions; one for each fold in the experiment. See sklearn.model_selection.KFold for
an example partitioner. (default = None, check the method description above to see how this affects the results)</p></li>
<li><p><strong>test_sample_ids</strong> – List of indicators for the samples to use for testing. e.g. [1,3], [True, False, True],
[‘human1’, ‘human3’], etc…, can also be pandas series or numpy array. (default = None, check the method
description above to see how this affects the results)</p></li>
<li><p><strong>start</strong> (<em>int</em>) – starting point of the grid search. (default: 5)</p></li>
<li><p><strong>end</strong> (<em>int</em>) – end point of the grid search. Use -1 to set end as the size of features (default: 100)</p></li>
<li><p><strong>jump</strong> (<em>int</em>) – gap between each sampled point in the grid (default: 5)</p></li>
<li><p><strong>max_processes</strong> (<em>int</em>) – this method uses ray package to create parallel process, this parameter defines the max
number of process to run (default: 10)</p></li>
<li><p><strong>verbose_frequency</strong> (<em>int</em>) – this parameter controls the frequency of progress outputs to console; an output is
printed to console after every verbose_frequency number of processes complete execution. (default: 10)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl>
<dt>contains 2 key-value pairs:</dt><dd><p>’optimal_n_results’: ndarray of shape (m, 2), with m being the total values sampled from the grid in the search.
The first column contains the number of top features (different values sampled from the grid search),  and the second
column contains the score.  The array is sorted by score in descending order.</p>
<p>’reduced_feature_ids’ : ndarray of shape (n, ), n is the smallest number of features, out of the m sampled values,
that produced the highest score. It contains reduced features ids (index of features_df).</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(dict)</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features_df</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_results&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span>  <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ranking_method_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attr&#39;</span><span class="p">:</span> <span class="s1">&#39;frequency&#39;</span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">partitioner</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.helper</span> <span class="k">as</span> <span class="nn">fhelper</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reduced_feature_results</span> <span class="o">=</span> <span class="n">fhelper</span><span class="o">.</span><span class="n">reduce_feature_set_size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
<span class="go">                        features_df,</span>
<span class="go">                        sample_ids_training,</span>
<span class="go">                        attrname,</span>
<span class="go">                        classifier,</span>
<span class="go">                        bsr,</span>
<span class="go">                        fhelper.rank_features_by_attribute,</span>
<span class="go">                        ranking_method_args,</span>
<span class="go">                        patitioner=partitioner,</span>
<span class="go">                        start = 5,</span>
<span class="go">                        end = 100,</span>
<span class="go">                        jump = 1,</span>
<span class="go">                        max_processes=10,</span>
<span class="go">                        verbose_frequency=10)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reduced_feature_results</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt id="datasci.sparse.feature_selection.helper.sliding_window_classification_on_ranked_features">
<code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.helper.</span></code><code class="sig-name descname"><span class="pre">sliding_window_classification_on_ranked_features</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">features_dataframe</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attr</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scorer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranking_method_handle</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranking_method_args</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partitioner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_sample_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose_limit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.helper.sliding_window_classification_on_ranked_features" title="Permalink to this definition">¶</a></dt>
<dd><p>This method takes a features dataframe (output of a feature selection), ranks them by a ranking method and performs
a sliding window classification experiment for various feature sets, defined by window size and stride. The result contains
score on various feature sets. Different training and test data may be used and the results change accordingly. The following choices are avaible:</p>
<p>if test_sample_ids is None and partitioner is None:</p>
<blockquote>
<div><p>Only training data is available, so the results will contain score on the Training data</p>
</div></blockquote>
<p>if test_sample_ids is not None and partitioner is None:</p>
<blockquote>
<div><p>Model is trained on all sample_ids, and then it is then evaluated on test_sample_ids. Results contain evaluation score on test_sample_ids.</p>
</div></blockquote>
<p>if test_sample_ids is None and partitioner is not None:</p>
<blockquote>
<div><p>Model is trained on partitions of sample_ids created by partitioner; results will contain the mean validation score,
obtained during validation on these different partitions.</p>
</div></blockquote>
<p>if test_sample_ids is not None and partitioner is not None:</p>
<blockquote>
<div><p>Model is trained using partitions of sample_ids defined by partitioner, and then the test_sample_ids are evaluated using all models. Results
contain mean evaluation score on test_sample_ids.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features_df</strong> (<em>pandas.DataFrame</em>) – This is a features dataframe that contains result of a feature selection.
(check datasci.core.dataset.DataSet.feature_select method for details)</p></li>
<li><p><strong>sample_ids</strong> (<em>like-like</em>) – List of indicators for the samples to use for training. e.g. [1,3], [True, False, True],
[‘human1’, ‘human3’], etc…, can also be pandas series or numpy array.</p></li>
<li><p><strong>attr</strong> (<em>string</em>) – Name of metadata attribute to classify on.</p></li>
<li><p><strong>classifier</strong> (<em>object</em>) – Classifier to run the classification experiment with; must have the sklearn equivalent
of a <code class="docutils literal notranslate"><span class="pre">fit</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> method.</p></li>
<li><p><strong>scorer</strong> (<em>object</em>) – Function which scores the prediction labels on training and test partitions. This function
should accept two arguments: truth labels and prediction labels. This function should output a score
between 0 and 1 which can be thought of as an accuracy measure. See
sklearn.metrics.balanced_accuracy_score for an example.</p></li>
<li><p><strong>ranking_method_handle</strong> (<em>method handle</em>) – handle of the feature ranking method</p></li>
<li><p><strong>ranking_method_args</strong> (<em>dict</em>) – argument dictionary for the feature ranking method</p></li>
<li><p><strong>partitioner</strong> (<em>object</em>) – Class-instance which partitions samples in batches of training and test split. This
instance must have the sklearn equivalent of a split method. The split method returns a list of
train-test partitions; one for each fold in the experiment. See sklearn.model_selection.KFold for
an example partitioner. (default = None, check the method description above to see how this affects the results)</p></li>
<li><p><strong>test_sample_ids</strong> – List of indicators for the samples to use for testing. e.g. [1,3], [True, False, True],
[‘human1’, ‘human3’], etc…, can also be pandas series or numpy array. (default = None, check the method
description above to see how this affects the results)</p></li>
<li><p><strong>= 50</strong> (<em>window_size</em>) – </p></li>
</ul>
</dd>
</dl>
<p>:param :
:param stride = 1:
:param :
:param max_processes: this method uses ray package to create parallel process, this parameter defines the max</p>
<blockquote>
<div><p>number of process to run (default: 10)</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>verbose_frequency</strong> (<em>int</em>) – this parameter controls the frequency of progress outputs to console; an output is
printed to console after every verbose_frequency number of processes complete execution. (default: 10)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The first column contains the starting position of the window,
and the second column contains the score.  The array is sorted by first column in ascending order.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(ndarray of shape (num_windows, 2))</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.core.dataset</span> <span class="k">as</span> <span class="nn">DS</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.IterativeFeatureRemoval</span> <span class="k">as</span> <span class="nn">IFR</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">DS</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifr</span> <span class="o">=</span> <span class="n">IFR</span><span class="o">.</span><span class="n">IFR</span><span class="p">(</span>
<span class="go">    verbosity = 2,</span>
<span class="go">    nfolds = 4,</span>
<span class="go">    repetition = 500,</span>
<span class="go">    cutoff = .6,</span>
<span class="go">    jumpratio = 5,</span>
<span class="go">    max_iters = 100,</span>
<span class="go">    max_features_per_iter_ratio = 2</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">feature_select</span><span class="p">(</span><span class="n">ifr</span><span class="p">,</span>
<span class="go">            attrname,</span>
<span class="go">            selector_name=&#39;IFR&#39;,</span>
<span class="go">            f_results_handle=&#39;results&#39;,</span>
<span class="go">            append_to_meta=False,</span>
<span class="go">            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features_df</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_results&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ranking_method_args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;attr&#39;</span><span class="p">:</span> <span class="s1">&#39;frequency&#39;</span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">partitioner</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">datasci.sparse.feature_selection.helper</span> <span class="k">as</span> <span class="nn">fhelper</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sliding_window_results</span> <span class="o">=</span> <span class="n">fhelper</span><span class="o">.</span><span class="n">sliding_window_classification_on_ranked_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
<span class="go">                        features_df,</span>
<span class="go">                        sample_ids_training,</span>
<span class="go">                        attrname,</span>
<span class="go">                        classifier,</span>
<span class="go">                        bsr,</span>
<span class="go">                        fhelper.rank_features_by_attribute,</span>
<span class="go">                        ranking_method_args,</span>
<span class="go">                        patitioner=partitioner,</span>
<span class="go">                        window_size = 50,</span>
<span class="go">                        stride = 5,</span>
<span class="go">                        verbose_limit=10,</span>
<span class="go">                        max_processes = 10)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sliding_window_results</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.kffs">
<span id="datasci-sparse-feature-selection-kffs-module"></span><h2>datasci.sparse.feature_selection.kffs module<a class="headerlink" href="#module-datasci.sparse.feature_selection.kffs" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.kffs.KFFS">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.kffs.</span></code><code class="sig-name descname"><span class="pre">KFFS</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classifier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_weights_handle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_rnk_func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>K-fold Feature Selection (kFFS) selects features using a classifier in a k-fold cross-validation experiment.
Specifically, the given classifier is trained on each fold, the features in each fold are ranked and sorted,
the top n features are selected from each fold, and the features across all folds are collected and ranked
by how many times a given feature was in the top n across each fold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k</strong> (<em>int</em>) – Indicates the number of folds to use in k-fold partition. Default is 5.</p></li>
<li><p><strong>n</strong> – (int): Indicates the rank threshold to use for each fold. KFFS grabs the top <code class="docutils literal notranslate"><span class="pre">n</span></code> features from each fold.</p></li>
<li><p><strong>classifier</strong> (<em>object</em>) – Class instance of the classifier being used, must contain a <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p></li>
<li><p><strong>f_weights_handle</strong> (<em>str</em>) – Name of the classifier attribute containing the feature weights for a given fold.</p></li>
<li><p><strong>f_rnk_func</strong> (<em>object</em>) – Function to be applied to feature weights for feature ranking. Default is None, and the
features will be ranked in from least to greatest.</p></li>
<li><p><strong>training_ids</strong> (<em>list</em>) – Optional list of training ids, to restrict feature selection further.</p></li>
<li><p><strong>random_state</strong> (<em>int</em>) – Random state to generate k-fold partitions. Default is 0.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.classifiers_">
<code class="sig-name descname"><span class="pre">classifiers_</span></code><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.classifiers_" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains each classifier trained on each fold.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>series</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.ranks_">
<code class="sig-name descname"><span class="pre">ranks_</span></code><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.ranks_" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains the final rankings of all the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.results_">
<code class="sig-name descname"><span class="pre">results_</span></code><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.results_" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains the feature weights and ranks for each fold, and the final rankings.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.kffs.KFFS.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kffs.KFFS.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the kFFS model to the training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>(</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training samples to be used for feature selection.</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>(</em><em>n_features</em><em>,</em><em>)</em>) – Training labels to be used for feature selection.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>inplace method. Results are stored in <a class="reference internal" href="#datasci.sparse.feature_selection.kffs.KFFS.classifiers_" title="datasci.sparse.feature_selection.kffs.KFFS.classifiers_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFFS.classifiers_</span></code></a>, <a class="reference internal" href="#datasci.sparse.feature_selection.kffs.KFFS.ranks_" title="datasci.sparse.feature_selection.kffs.KFFS.ranks_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFFS.ranks_</span></code></a>, and <a class="reference internal" href="#datasci.sparse.feature_selection.kffs.KFFS.results_" title="datasci.sparse.feature_selection.kffs.KFFS.results_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFFS.results_</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.kfold_loso_ifr">
<span id="datasci-sparse-feature-selection-kfold-loso-ifr-module"></span><h2>datasci.sparse.feature_selection.kfold_loso_ifr module<a class="headerlink" href="#module-datasci.sparse.feature_selection.kfold_loso_ifr" title="Permalink to this headline">¶</a></h2>
<p>This module contains code for implementing an iterative feature removal (IFR) algorithm using leave one
subject out (LOSO) partitions.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.kfold_loso_ifr.</span></code><code class="sig-name descname"><span class="pre">KFLIFR</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">classifier</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">object</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_handle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_splits_kfold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state_kfold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_test_splits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_top_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jump_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_freq_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imputer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>This iterative feature removal (IFR) algorithm produces ranked feature sets from a single classifier and a data set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>classifier</strong> (<em>object</em>) – The classifier used to select features. The classifier should produce feature weights and ideally
be sparse, so that relatively few features are weighted heavily compared to the total.</p></li>
<li><p><strong>weights_handle</strong> (<em>str</em>) – The name of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.classifier</span></code> attribute where the weights are stored. The
weights stored there should be an ndarray.</p></li>
<li><p><strong>n_splits_kfold</strong> (<em>int</em>) – The number of folds, <code class="docutils literal notranslate"><span class="pre">k</span></code> used in the k-fold cross validation.</p></li>
<li><p><strong>random_state_kfold</strong> (<em>int</em>) – The random seed used in generating the k-fold partitions. Good for reproducibility
of results. The default is None.</p></li>
<li><p><strong>train_test_splits</strong> (<em>list</em>) – An alternate list of (training, test) splits that replace the k-fold cross validation used
in the algorithm. This is useful if you have a specific set of splits you want to use. The default is None,
but if provided the arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.random_state_kfold</span></code> are
ignored.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – The proportion of features used to break the IFR loop. The IFR loop will stop once the number of
features extracted reaches this proportion of the total features.</p></li>
<li><p><strong>n_top_features</strong> (<em>int</em>) – The number of top features to remove at each iteration of IFR. If this parameter is not
given then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.jump_ratio</span></code> will be used instead. The default is None.</p></li>
<li><p><strong>jump_ratio</strong> (<em>float</em>) – The weight ratio used to determine the number of top features to remove for each iteration
of IFR. For example let <span class="math notranslate nohighlight">\(r_i = w_{i}/w_{i+1}\)</span> denote the ratio of the
<span class="math notranslate nohighlight">\(i\)</span> th largest weight and <span class="math notranslate nohighlight">\(i+1\)</span> largest weight, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.jump_ratio</span></code> = 2 means
that top features will be chosen until <span class="math notranslate nohighlight">\(r_i \geq 2\)</span>. If this parameter is not given then the user
must provide a constant number of features to prune at each step via the parameter <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.n_top_features</span></code>.</p></li>
<li><p><strong>sort_freq_classes</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> the algorithm will sort frequency classes of feature by the mean of
normalized weight across a LOSO experiment— providing a unique ranking. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>imputer</strong> (<em>object</em>) – Optional imputer to impute training set values with.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_">
<code class="sig-name descname"><span class="pre">results_</span></code><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_" title="Permalink to this definition">¶</a></dt>
<dd><p>Outputs the feature frequencies and rankings for each fold provided by the
k-fold partition defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.random_state_kfold</span></code>, or
the user defined splits defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.train_test_splits</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the feature selection algorithm and stores the results in the attribute <a class="reference internal" href="#datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_" title="datasci.sparse.feature_selection.kfold_loso_ifr.KFLIFR.results_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.results_</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The data array used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.classifier</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – The labels used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFLIFR.classifier</span></code>.</p></li>
<li><p><strong>groups</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – Optional set up of labels defining the groups used in a
Leave-One-Group-Out experiment. This is useful if you don’t want members of the same group in both the
training in test for a LOSO fold.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>inplace method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.kfold_shuffle_ifr">
<span id="datasci-sparse-feature-selection-kfold-shuffle-ifr-module"></span><h2>datasci.sparse.feature_selection.kfold_shuffle_ifr module<a class="headerlink" href="#module-datasci.sparse.feature_selection.kfold_shuffle_ifr" title="Permalink to this headline">¶</a></h2>
<p>This module contains code for implementing an iterative feature removal (IFR) algorithm using leave one
subject out (LOSO) partitions.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.kfold_shuffle_ifr.</span></code><code class="sig-name descname"><span class="pre">KFSIFR</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">classifier</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">object</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_handle</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_splits_kfold</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state_kfold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_test_splits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_feature_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_splits_shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state_shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_prop_shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_top_features</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jump_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_freq_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imputer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>This iterative feature removal (IFR) algorithm produces ranked feature sets from a single classifier and a data set.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>classifier</strong> (<em>object</em>) – The classifier used to select features. The classifier should produce feature weights and ideally
be sparse, so that relatively few features are weighted heavily compared to the total.</p></li>
<li><p><strong>weights_handle</strong> (<em>str</em>) – The name of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.classifier</span></code> attribute where the weights are stored. The
weights stored there should be an ndarray.</p></li>
<li><p><strong>n_splits_kfold</strong> (<em>int</em>) – The number of folds, <code class="docutils literal notranslate"><span class="pre">k</span></code> used in the k-fold cross validation.</p></li>
<li><p><strong>random_state_kfold</strong> (<em>int</em>) – The random seed used in generating the k-fold partitions. Good for reproducibility
of results. The default is None.</p></li>
<li><p><strong>train_test_splits</strong> (<em>list</em>) – An alternate list of (training, test) splits that replace the k-fold cross validation used
in the algorithm. This is useful if you have a specific set of splits you want to use. The default is None,
but if provided the arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.random_state_kfold</span></code> are
ignored.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – Classification rate used to break the IFR loop. The IFR loop will stop once the number of
features extracted reaches this proportion of the total features.</p></li>
<li><p><strong>max_feature_threshold</strong> (<em>int</em>) – The maximum number of features that may be removed on an iteration of IFR.
Default is None.</p></li>
<li><p><strong>n_splits_shuffle</strong> (<em>int</em>) – The number of shuffle splits to use for the inner loop. Default is 100.</p></li>
<li><p><strong>random_state_shuffle</strong> (<em>int</em>) – Random seed for shuffle splits.</p></li>
<li><p><strong>n_top_features</strong> (<em>int</em>) – The number of top features to remove at each iteration of IFR. If this parameter is not
given then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.jump_ratio</span></code> will be used instead. The default is None.</p></li>
<li><p><strong>jump_ratio</strong> (<em>float</em>) – The weight ratio used to determine the number of top features to remove for each iteration
of IFR. For example let <span class="math notranslate nohighlight">\(r_i = w_{i}/w_{i+1}\)</span> denote the ratio of the
<span class="math notranslate nohighlight">\(i\)</span> th largest weight and <span class="math notranslate nohighlight">\(i+1\)</span> largest weight, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.jump_ratio</span></code> = 2 means
that top features will be chosen until <span class="math notranslate nohighlight">\(r_i \geq 2\)</span>. If this parameter is not given then the user
must provide a constant number of features to prune at each step via the parameter <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.n_top_features</span></code>.</p></li>
<li><p><strong>sort_freq_classes</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> the algorithm will sort frequency classes of feature by the mean of
normalized weight across a LOSO experiment— providing a unique ranking. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>imputer</strong> (<em>object</em>) – Optional imputer to impute training set values with.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_">
<code class="sig-name descname"><span class="pre">results_</span></code><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_" title="Permalink to this definition">¶</a></dt>
<dd><p>Outputs the feature frequencies and rankings for each fold provided by the
k-fold partition defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.n_splits_kfold</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.random_state_kfold</span></code>, or
the user defined splits defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.train_test_splits</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the feature selection algorithm and stores the results in the attribute <a class="reference internal" href="#datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_" title="datasci.sparse.feature_selection.kfold_shuffle_ifr.KFSIFR.results_"><code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.results_</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The data array used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.classifier</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – The labels used to select features via <code class="xref py py-attr docutils literal notranslate"><span class="pre">KFSIFR.classifier</span></code>.</p></li>
<li><p><strong>groups</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>)</em>) – Optional set up of labels defining the groups used in a
Leave-One-Group-Out experiment. This is useful if you don’t want members of the same group in both the
training in test for a LOSO fold.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>inplace method.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.pathway_selection">
<span id="datasci-sparse-feature-selection-pathway-selection-module"></span><h2>datasci.sparse.feature_selection.pathway_selection module<a class="headerlink" href="#module-datasci.sparse.feature_selection.pathway_selection" title="Permalink to this headline">¶</a></h2>
<p>Module containing classes for pathway discovery in -omics data.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.pathway_selection.</span></code><code class="sig-name descname"><span class="pre">PathwayScore</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ratio'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore.convert_type">
<code class="sig-name descname"><span class="pre">convert_type</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore.convert_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pathways</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pathway_selection.PathwayScore.transform">
<code class="sig-name descname"><span class="pre">transform</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pathway_selection.PathwayScore.transform" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.pca_snr_selection">
<span id="datasci-sparse-feature-selection-pca-snr-selection-module"></span><h2>datasci.sparse.feature_selection.pca_snr_selection module<a class="headerlink" href="#module-datasci.sparse.feature_selection.pca_snr_selection" title="Permalink to this headline">¶</a></h2>
<p>This module contains a class which implements the PCA-SNR feature selection method.</p>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.pca_snr_selection.</span></code><code class="sig-name descname"><span class="pre">PCASNR</span></code><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR.plot_snrs">
<code class="sig-name descname"><span class="pre">plot_snrs</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR.plot_snrs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="datasci.sparse.feature_selection.pca_snr_selection.PCASNR.snr">
<code class="sig-name descname"><span class="pre">snr</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.pca_snr_selection.PCASNR.snr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-datasci.sparse.feature_selection.sabs">
<span id="datasci-sparse-feature-selection-sabs-module"></span><h2>datasci.sparse.feature_selection.sabs module<a class="headerlink" href="#module-datasci.sparse.feature_selection.sabs" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="datasci.sparse.feature_selection.sabs.SABS">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">datasci.sparse.feature_selection.sabs.</span></code><code class="sig-name descname"><span class="pre">SABS</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_updates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cuda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.sabs.SABS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<dl class="py method">
<dt id="datasci.sparse.feature_selection.sabs.SABS.fit">
<code class="sig-name descname"><span class="pre">fit</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#datasci.sparse.feature_selection.sabs.SABS.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="datasci.sparse.classifiers.html" class="btn btn-neutral float-left" title="datasci.sparse.classifiers namespace" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Eric Kehoe, Kartikay Sharma.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>